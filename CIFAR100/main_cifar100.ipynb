{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1665575967323,"user":{"displayName":"Shiv Ram Duubey","userId":"00955864886781257375"},"user_tz":-330},"id":"v_fG538P6N4l","outputId":"22108fe7-ce84-45f2-c759-7de9af28215b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Oct 12 12:01:22 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   66C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4489,"status":"ok","timestamp":1665575971810,"user":{"displayName":"Shiv Ram Duubey","userId":"00955864886781257375"},"user_tz":-330},"id":"iYAvgeVDRXYG","outputId":"83ef456d-5c1d-41a3-b5b6-e61105b4a16c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1665575971811,"user":{"displayName":"Shiv Ram Duubey","userId":"00955864886781257375"},"user_tz":-330},"id":"LLyFOsSoByip","outputId":"e2eafd2b-659e-4914-cea5-776d23858750"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/EXP/AdaNorm/CIFAR100\n"]}],"source":["cd '/content/gdrive/My Drive/EXP/AdaNorm/CIFAR100/'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MY9oaZ5cQt9D","outputId":"1e9a7af9-3358-46bd-9bb4-48563b6b5370","executionInfo":{"status":"ok","timestamp":1665576040335,"user_tz":-330,"elapsed":68533,"user":{"displayName":"Shiv Ram Duubey","userId":"00955864886781257375"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model..\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","\n","Epoch: 0\n","/content/gdrive/My Drive/EXP/AdaNorm/CIFAR100/AdaBeliefNorm.py:155: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad1)\n"," [================================================================>]  Step: 266ms | Tot: 37s389ms | Loss: 4.601 | Acc: 1.500% (750/50000) 782/782 \n"," [================================================================>]  Step: 6ms | Tot: 2s649ms | Loss: 4.441 | Acc: 2.080% (208/10000) 157/157 \n","Saving..\n","\n","Epoch: 1\n","Traceback (most recent call last):\n","  File \"main_cifar100.py\", line 167, in <module>\n","    train(epoch)\n","  File \"main_cifar100.py\", line 101, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n"]}],"source":["!python3 main_cifar100.py\n","#!python3 main_cifar100.py --resume --lr=0.001\n","#!python3 main_cifar100.py --resume --lr=0.0001"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}